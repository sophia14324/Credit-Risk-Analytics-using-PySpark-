{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, isnan\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditScoringSpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"loan_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "print(f\"PySpark Version: {pyspark_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `loan_status` cannot be resolved. Did you mean one of the following? [`int.rate`, `dti`, `fico`, `installment`, `pub.rec`].;\n'Project [credit.policy#17, purpose#18, int.rate#19, installment#20, log.annual.inc#21, dti#22, fico#23, days.with.cr.line#24, revol.bal#25, revol.util#26, inq.last.6mths#27, delinq.2yrs#28, pub.rec#29, not.fully.paid#30, CASE WHEN 'loan_status IN (Charged Off,Default,Late) THEN 1 ELSE 0 END AS loan_status_binary#45]\n+- Relation [credit.policy#17,purpose#18,int.rate#19,installment#20,log.annual.inc#21,dti#22,fico#23,days.with.cr.line#24,revol.bal#25,revol.util#26,inq.last.6mths#27,delinq.2yrs#28,pub.rec#29,not.fully.paid#30] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m label_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_status_binary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 3. Basic transformations: turning loan_status into binary\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# e.g., 1 for bad loans, 0 for good loans\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloan_status\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCharged Off\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Drop missing critical rows\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [label_col] \u001b[38;5;241m+\u001b[39m numeric_cols:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[1;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `loan_status` cannot be resolved. Did you mean one of the following? [`int.rate`, `dti`, `fico`, `installment`, `pub.rec`].;\n'Project [credit.policy#17, purpose#18, int.rate#19, installment#20, log.annual.inc#21, dti#22, fico#23, days.with.cr.line#24, revol.bal#25, revol.util#26, inq.last.6mths#27, delinq.2yrs#28, pub.rec#29, not.fully.paid#30, CASE WHEN 'loan_status IN (Charged Off,Default,Late) THEN 1 ELSE 0 END AS loan_status_binary#45]\n+- Relation [credit.policy#17,purpose#18,int.rate#19,installment#20,log.annual.inc#21,dti#22,fico#23,days.with.cr.line#24,revol.bal#25,revol.util#26,inq.last.6mths#27,delinq.2yrs#28,pub.rec#29,not.fully.paid#30] csv\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = [\"loan_amnt\", \"annual_inc\", \"dti\"]\n",
    "categorical_cols = [\"grade\", \"home_ownership\", \"purpose\"]\n",
    "label_col = \"loan_status_binary\"\n",
    "\n",
    "data = data.withColumn(\n",
    "    label_col,\n",
    "    when(col(\"loan_status\").isin([\"Charged Off\",\"Default\",\"Late\"]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "for c in [label_col] + numeric_cols:\n",
    "    data = data.filter((col(c).isNotNull()) & (~isnan(col(c))))\n",
    "\n",
    "indexers = []\n",
    "for cat_col in categorical_cols:\n",
    "    indexers.append(\n",
    "        StringIndexer(inputCol=cat_col, outputCol=cat_col + \"_indexed\", handleInvalid=\"skip\")\n",
    "    )\n",
    "\n",
    "assembler_inputs = [c + \"_indexed\" for c in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"raw_features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=label_col,\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100])\n",
    "             .addGrid(rf.maxDepth, [5, 10, None])\n",
    "             .addGrid(rf.minInstancesPerNode, [1, 5])\n",
    "             .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3,\n",
    "                    parallelism=2)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test ROC-AUC: {auc}\")\n",
    "\n",
    "predictions_pd = predictions.select(\"prediction\",\"probability\",label_col).toPandas()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display schema and first few rows\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "\n",
    "# Drop irrelevant columns (as an example)\n",
    "data = data.drop(\"id\", \"member_id\", \"url\", \"desc\")\n",
    "\n",
    "# Handle missing values: Fill or drop\n",
    "data = data.na.fill({\n",
    "    \"annual_inc\": 0,\n",
    "    \"dti\": 0,\n",
    "    \"loan_amnt\": 0,\n",
    "    \"funded_amnt\": 0,\n",
    "    \"funded_amnt_inv\": 0,\n",
    "    \"total_pymnt\": 0,\n",
    "    \"total_rec_int\": 0\n",
    "})\n",
    "\n",
    "# Convert categorical variables to numeric\n",
    "data = data.withColumn(\"home_ownership\", when(col(\"home_ownership\") == \"RENT\", 1)\n",
    "                       .when(col(\"home_ownership\") == \"OWN\", 2)\n",
    "                       .when(col(\"home_ownership\") == \"MORTGAGE\", 3)\n",
    "                       .otherwise(0))\n",
    "\n",
    "# Clean up any string columns if needed\n",
    "data = data.withColumn(\"purpose\", regexp_replace(col(\"purpose\"), \" \", \"_\")) \\\n",
    "           .withColumn(\"purpose\", trim(col(\"purpose\")))\n",
    "\n",
    "# Show cleaned data\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Example: Create Debt-to-Income (DTI) Ratio\n",
    "data = data.withColumn(\"dti_ratio\", col(\"dti\") / 100)\n",
    "\n",
    "# Create Loan-to-Value (LTV) ratio\n",
    "data = data.withColumn(\"ltv_ratio\", col(\"loan_amnt\") / (col(\"home_value\") * 0.8))  # Assuming home_value is provided\n",
    "\n",
    "# Drop any rows with invalid LTV ratio if necessary\n",
    "data = data.filter(col(\"ltv_ratio\").isNotNull())\n",
    "\n",
    "# Select relevant features for modeling\n",
    "features = data.select(\"loan_amnt\", \"annual_inc\", \"dti\", \"home_ownership\", \"purpose\", \"dti_ratio\", \"ltv_ratio\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\"loan_amnt\", \"annual_inc\", \"dti_ratio\", \"home_ownership\"], outputCol=\"features\")\n",
    "data = assembler.transform(features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"default\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"default\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'credit_risk_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
